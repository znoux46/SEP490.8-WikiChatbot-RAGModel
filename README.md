# ğŸš€ RAG System - Production-Ready Microservice

Há»‡ thá»‘ng RAG (Retrieval-Augmented Generation) Ä‘Æ°á»£c Ä‘Ã³ng gÃ³i thÃ nh **Production-Ready Microservice** vá»›i FastAPI, PostgreSQL (pgvector), Redis Queue vÃ  Docker.

## ğŸŒŸ Highlights

- âœ… **RESTful API** vá»›i FastAPI
- âœ… **Vector Database** vá»›i PostgreSQL + pgvector
- âœ… **Background Processing** vá»›i Redis Queue (RQ)
- âœ… **Multi-file Upload** - Upload nhiá»u files cÃ¹ng lÃºc qua multipart-form data
- âœ… **Duplicate Detection** - SHA256 hash Ä‘á»ƒ trÃ¡nh trÃ¹ng láº·p
- âœ… **Batch Tracking** - Redis tracking tá»•ng thá»i gian xá»­ lÃ½ cá»§a batch files
- âœ… **Docker Ready** - docker-compose Ä‘á»ƒ deploy má»™t lá»‡nh
- âœ… **Auto Documentation** - Swagger UI tÃ­ch há»£p sáºµn
- âœ… **Performance Logging** - Chi tiáº¿t timing tá»«ng phase (Ingest, Chunking, Embedding, Database)

## ğŸš€ Quick Start

### 1. Setup Environment

```bash
# Copy environment template
copy .env.example .env

# Edit .env vÃ  thÃªm:
# GEMINI_API_KEY=your_gemini_api_key_here
# GROQ_API_KEY=your_groq_api_key_here
# EMBEDDING_MODEL_NAME=models/text-embedding-004
```

### 2. Start Services

```bash
docker-compose up -d --build
```

Há»‡ thá»‘ng sáº½ khá»Ÿi Ä‘á»™ng:

- **FastAPI** (port 8000)
- **PostgreSQL** vá»›i pgvector (port 5432)
- **Redis** (port 6379)
- **RQ Worker** (background processing)

### 3. Access API

Má»Ÿ browser: **http://localhost:8000/docs**

**Xong!** ğŸ‰ API Ä‘Ã£ sáºµn sÃ ng.

## ï¿½ Database Migrations

Khi cáº§n cháº¡y migration cho database:

### CÃ¡ch 1: PowerShell (Windows)

```powershell
Get-Content migrations/add_file_size_and_hash.sql | docker exec -i rag_postgres psql -U rag_user -d rag_db
```

### CÃ¡ch 2: Bash (Linux/Mac)

```bash
cat migrations/add_file_size_and_hash.sql | docker exec -i rag_postgres psql -U rag_user -d rag_db
```

### CÃ¡ch 3: Trá»±c tiáº¿p trong container

```bash
docker exec -i rag_postgres psql -U rag_user -d rag_db -f /migrations/add_file_size_and_hash.sql
```

> **ğŸ’¡ Tip**: Migration files náº±m trong thÆ° má»¥c `migrations/`. Cháº¡y theo thá»© tá»± tá»« cÅ© Ä‘áº¿n má»›i.

## ğŸ“š API Endpoints

### 1. Upload & Process Documents

**POST** `/api/v1/process`

Upload má»™t hoáº·c nhiá»u files (HTML) Ä‘á»ƒ xá»­ lÃ½:

```bash
curl -X POST "http://localhost:8000/api/v1/process" \
  -H "Content-Type: multipart/form-data" \
  -F "files=@file1.html" \
  -F "files=@file2.html" \
  -F "chunk_size=800" \
  -F "chunk_overlap=150"
```

**Features**:

- âœ… Multi-file upload
- âœ… Content-length validation (max 50MB)
- âœ… SHA256 duplicate detection
- âœ… Background processing vá»›i RQ
- âœ… Batch timing tracking

**Response**:

```json
{
  "total_files": 2,
  "results": [
    {
      "filename": "file1.html",
      "status": "processing",
      "job_id": "job_abc123",
      "document_id": 1,
      "message": "File uploaded successfully"
    }
  ]
}
```

### 2. Check Job Status

**GET** `/api/v1/jobs/{job_id}/status`

```bash
curl http://localhost:8000/api/v1/jobs/job_abc123/status
```

### 3. Search Documents

**POST** `/api/v1/search`

```bash
curl -X POST "http://localhost:8000/api/v1/search" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Há»“ ChÃ­ Minh sinh nÄƒm nao",
    "top_k": 5
  }'
```

### 4. RAG Chat

**POST** `/api/v1/chat`

```bash
curl -X POST "http://localhost:8000/api/v1/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Há»“ ChÃ­ Minh sinh nÄƒm nao",
    "top_k": 10
  }'
```

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Client (Browser/API)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   FastAPI        â”‚ â† Port 8000
                    â”‚   (REST API)     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚            â”‚            â”‚
                â–¼            â–¼            â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚PostgreSQLâ”‚  â”‚  Redis  â”‚  â”‚ Worker  â”‚
         â”‚+pgvector â”‚  â”‚  Queue  â”‚  â”‚  (RQ)   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚             â”‚             â”‚
              â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚           Queue Jobs
              â”‚
         â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
         â”‚          â”‚
    Documents    Chunks
    (Metadata)   (Vectors)
```

**Data Flow**:

1. Client upload file(s) â†’ FastAPI
2. FastAPI lÆ°u temp file, táº¡o document record, queue job
3. Worker nháº­n job tá»« Redis Queue
4. Worker: Ingest â†’ Chunking â†’ Embedding â†’ Save to PostgreSQL
5. Worker update batch tracking trong Redis
6. Worker cuá»‘i cÃ¹ng log tá»•ng thá»i gian batch

## ğŸ”§ Tech Stack

- **API Framework**: FastAPI
- **Vector DB**: PostgreSQL 17 + pgvector
- **Queue**: Redis + RQ (Redis Queue)
- **Embedding**: Google Gemini API (text-embedding-004)
- **LLM**: Groq API (llama3-70b-8192)
- **File Processing**: BeautifulSoup4, MarkItDown
- **Deployment**: Docker + Docker Compose

## ğŸ“Š Performance Monitoring

Há»‡ thá»‘ng tá»± Ä‘á»™ng log timing cho tá»«ng phase:

```
======================================================================
âœ… PROCESSING COMPLETED - Summary
======================================================================
ğŸ“Š Document ID: 123
ğŸ“Š Total chunks created: 45
â±ï¸  TOTAL TIME: 12.34s

ğŸ“ˆ Time Breakdown:
   â€¢ Ingest:    3.21s (26.0%)
   â€¢ Chunking:  2.10s (17.0%)
   â€¢ Embedding: 5.89s (47.7%)
   â€¢ Database:  1.14s (9.2%)
======================================================================
```

**Batch Processing Log**:

```
======================================================================
ğŸ‰ BATCH COMPLETED - All 3 file(s) processed
======================================================================
ğŸ“Š Batch ID: batch_a1b2c3d4e5f6
â±ï¸  TOTAL BATCH TIME: 45.67s

ğŸ“ˆ Total Time Breakdown (All Files):
   â€¢ Ingest:    12.34s (27.0%)
   â€¢ Chunking:  8.56s (18.7%)
   â€¢ Embedding: 21.45s (47.0%)
   â€¢ Database:  3.32s (7.3%)

âš¡ Average per file: 15.22s
======================================================================
```

## ğŸ”„ Original Script (Legacy)

> **Note**: Script tÆ°Æ¡ng tÃ¡c cÅ© váº«n cÃ³ táº¡i `src/main.py` (dÃ¹ng Pinecone) nhÆ°ng **khÃ´ng khuyáº¿n nghá»‹** sá»­ dá»¥ng. HÃ£y dÃ¹ng API microservice má»›i.

## ğŸ“ Project Structure

```
RAG/
â”œâ”€â”€ app/                           # Microservice source code
â”‚   â”œâ”€â”€ api/                       # API routes & schemas
â”‚   â”‚   â”œâ”€â”€ routes.py              # REST endpoints
â”‚   â”‚   â””â”€â”€ schemas.py             # Pydantic models
â”‚   â”œâ”€â”€ database/                  # Database layer
â”‚   â”‚   â”œâ”€â”€ models.py              # SQLAlchemy models
â”‚   â”‚   â””â”€â”€ connection.py          # DB connection
â”‚   â”œâ”€â”€ services/                  # Business logic
â”‚   â”‚   â”œâ”€â”€ chunking_service.py    # Text chunking
â”‚   â”‚   â”œâ”€â”€ embedding_service.py   # Vector embeddings
â”‚   â”‚   â”œâ”€â”€ queue_service.py       # Redis queue
â”‚   â”‚   â”œâ”€â”€ search_service.py      # Vector search
â”‚   â”‚   â””â”€â”€ rag_service.py         # RAG pipeline
â”‚   â”œâ”€â”€ workers/                   # Background workers
â”‚   â”‚   â””â”€â”€ process_worker.py      # Document processing
â”‚   â”œâ”€â”€ config.py                  # Configuration
â”‚   â””â”€â”€ main.py                    # FastAPI app
â”œâ”€â”€ migrations/                    # SQL migrations
â”‚   â””â”€â”€ add_file_size_and_hash.sql
â”œâ”€â”€ data/
â”‚   â””â”€â”€ temp/                      # Temporary upload files
â”‚       â””â”€â”€ .gitkeep
â”œâ”€â”€ docker-compose.yml             # Docker orchestration
â”œâ”€â”€ Dockerfile                     # API container
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ .env.example                   # Environment template
â””â”€â”€ README.md
```

## âš™ï¸ Configuration

File `.env` cáº§n cÃ³:

```env
# API Keys
GEMINI_API_KEY=your_gemini_api_key_here

# Database
POSTGRES_USER=rag_user
POSTGRES_PASSWORD=rag_password
POSTGRES_DB=rag_db
DATABASE_URL=postgresql://rag_user:rag_password@postgres:5432/rag_db

# Redis
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_DB=0
```

### Chunking Parameters

Trong API request, báº¡n cÃ³ thá»ƒ Ä‘iá»u chá»‰nh:

```bash
curl -X POST "http://localhost:8000/api/v1/process" \
  -F "files=@file.html" \
  -F "chunk_size=800" \      # KÃ­ch thÆ°á»›c chunk (chars)
  -F "chunk_overlap=150"     # Overlap giá»¯a chunks (chars)
```

**Khuyáº¿n nghá»‹**:

- `chunk_size`: 600-1000 chars
- `chunk_overlap`: 100-200 chars (15-20% cá»§a chunk_size)

## ğŸ› Troubleshooting

### 1. Container khÃ´ng start

```bash
# Check logs
docker-compose logs -f api
docker-compose logs -f postgres
docker-compose logs -f worker

# Restart services
docker-compose restart
```

### 2. Migration chÆ°a cháº¡y

```bash
# Cháº¡y migration
Get-Content migrations/add_file_size_and_hash.sql | docker exec -i rag_postgres psql -U rag_user -d rag_db
```

### 3. Worker khÃ´ng xá»­ lÃ½ jobs

```bash
# Check worker logs
docker-compose logs -f worker

# Restart worker
docker-compose restart worker
```

### 4. File upload quÃ¡ 50MB

```
âŒ Error: Request quÃ¡ lá»›n. Tá»‘i Ä‘a 50MB
```

**Giáº£i phÃ¡p**: TÄƒng giá»›i háº¡n trong [routes.py](app/api/routes.py) hoáº·c split file nhá» hÆ¡n.

### 5. Duplicate file detected

```json
{
  "status": "duplicate",
  "message": "File already exists",
  "document_id": 123
}
```

**LÃ½ do**: SHA256 hash trÃ¹ng vá»›i document hiá»‡n cÃ³ (cÃ¹ng ná»™i dung).

## ğŸ“Š Monitoring

### Check Services Health

```bash
# API health check
curl http://localhost:8000/health

# Check PostgreSQL
docker exec rag_postgres psql -U rag_user -d rag_db -c "SELECT COUNT(*) FROM documents;"

# Check Redis queue
docker exec rag_redis redis-cli LLEN rq:queue:process
```

### View Logs

```bash
# Real-time logs
docker-compose logs -f

# Specific service
docker-compose logs -f api
docker-compose logs -f worker
```

### Database Queries

```bash
# Connect to PostgreSQL
docker exec -it rag_postgres psql -U rag_user -d rag_db

# Example queries
SELECT id, title, status FROM documents;
SELECT COUNT(*) FROM chunks;
SELECT COUNT(*) FROM chunks WHERE document_id = 1;
```

## ğŸš€ Production Deployment

### Environment Variables

Táº¡o `.env.production`:

```env
GEMINI_API_KEY=prod_key_here
POSTGRES_PASSWORD=strong_password_here
DATABASE_URL=postgresql://user:pass@prod-db:5432/rag_db
```

### Docker Compose Production

```bash
# Build production images
docker-compose -f docker-compose.prod.yml build

# Start with production config
docker-compose -f docker-compose.prod.yml up -d
```

### Scaling Workers

```bash
# Scale to 3 workers
docker-compose up -d --scale worker=3
```

## ğŸ¯ Development

### Local Development (Without Docker)

```bash
# Install dependencies
pip install -r requirements.txt

# Start PostgreSQL & Redis (Docker)
docker-compose up -d postgres redis

# Run API locally
uvicorn app.main:app --reload --port 8000

# Run worker locally
rq worker process --url redis://localhost:6379/0
```

### Run Tests

```bash
# TODO: Add tests
pytest tests/
```

## ğŸ“ˆ Performance Tips

1. **TÄƒng retrieval quality**:
   - TÄƒng `top_k` lÃªn 15-20
   - Giáº£m `chunk_size` xuá»‘ng 600

2. **Giáº£m processing time**:
   - Scale workers: `docker-compose up -d --scale worker=3`
   - Tá»‘i Æ°u chunk_size vÃ  overlap

3. **Monitoring batch jobs**:
   - Xem worker logs Ä‘á»ƒ track batch timing
   - Redis batch tracking tá»± Ä‘á»™ng cleanup sau 24h

## ğŸ“ Support

Náº¿u gáº·p váº¥n Ä‘á»:

1. âœ… Check logs: `docker-compose logs -f`
2. âœ… Verify `.env` cÃ³ Ä‘áº§y Ä‘á»§ API keys
3. âœ… Äáº£m báº£o migrations Ä‘Ã£ cháº¡y
4. âœ… Check services Ä‘ang cháº¡y: `docker-compose ps`
5. âœ… Restart services: `docker-compose restart`
